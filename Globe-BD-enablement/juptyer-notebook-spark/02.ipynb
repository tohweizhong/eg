{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using IntelliJ IDEA for Spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change the directory to your ./code directory and use your favorite editor -"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd /usr/Local/Cellar/spark/code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and use IntelliJ IDEA for Scala and Python code submissions.  Download from [here](https://www.jetbrains.com/idea/download/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/intellj.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the IntelliJ IDEA Scala Plug-In from [here](https://plugins.jetbrains.com/plugin/1347?pr=idea)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the Scala IntelliJ IDEA Plug-In via the IntelliJ IDEA and the direct download is [here](https://plugins.jetbrains.com/files/1347/25886/scala-intellij-bin-3.0.6.zip)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/scala_plugin.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create A New Project\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/17.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/18.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mkdir /usr/Local/Cellar/spark/project"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/19.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/20.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/21.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/24.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/25.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/26.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/27.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/28.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/29.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/30.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the Scala Spark Skeleton\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mkdir /usr/Local/Cellar/spark/sandbox/sparkgrep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Download the Scala Spark Skeleton from [here](https://gist.github.com/mhausenblas/9f8ac25d2b4f94d07c99) and extract into the ./sandbox"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mv SparkGrep.scala /usr/Local/Cellar/spark/sandbox/sparkgrep/SparkGrep.scala"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mv pom.xml /usr/Local/Cellar/spark/sandbox/sparkgrep/pom.xml"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import the base directory (File → Import Project), wait a bit until everything is imported and then select the file SparkGrep.scala, right click it and choose ‘Run SparkGrep.scala’. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We’ll create a very simple Spark application in Scala–so simple, in fact, that it’s named SimpleApp.scala:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "/* SimpleApp.scala */\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "object SimpleApp {\n",
    "  def main(args: Array[String]) {\n",
    "    val logFile = \"/usr/Local/Cellar/spark/1.6.2/README.md\" // Should be some file on your system\n",
    "    val conf = new SparkConf().setAppName(\"Simple Application\")\n",
    "    val sc = new SparkContext(conf)\n",
    "    val logData = sc.textFile(logFile, 2).cache()\n",
    "    val numAs = logData.filter(line => line.contains(\"a\")).count()\n",
    "    val numBs = logData.filter(line => line.contains(\"b\")).count()\n",
    "    println(\"Lines with a: %s, Lines with b: %s\".format(numAs, numBs))\n",
    "  }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program just counts the number of lines containing ‘a’ and the number containing ‘b’ in the Spark README. Note that you’ll need to replace YOUR_SPARK_HOME with the location where Spark is installed. Unlike the earlier examples with the Spark shell, which initializes its own SparkContext, we initialize a SparkContext as part of the program."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " scalac SimpleApp.scala\n",
    " scala SimpleApp"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
