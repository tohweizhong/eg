{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install and Configuration of Spark 1.6.2 (Mac OS X & Linux)\n",
    "Setting Up On A Stand Alone Machine\n",
    "--"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For <b>Mac OS X and Linux</b> users, use the Brew and Linuxbrew directory structure.  But DO NOT install via Brew or Linuxbrew as the version is Spark 1.0.1.  Create the following directory -"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "mkdir /usr/Local/Cellar/spark/1.6.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "However, if you have already pre-installed Spark via Brew or Linuxbrew, run this command - "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "brew remove spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## tar -xvzf spark-1.6.2-bin-hadoop2.6.tgz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/spark_mac.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Copy Spark environment variables script file from template file.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd spark\n",
    "mkdir data"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd conf\n",
    "vi spark-env.sh"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "export SPARK_WORKER_MEMORY=1g\n",
    "export SPARK_EXECUTOR_MEMORY=512m\n",
    "export SPARK_WORKER_INSTANCES=2\n",
    "export SPARK_WORKER_CORES=2\n",
    "export SPARK_WORKER_DIR=/usr/Local/Cellar/spark/data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SPARK_WORKER_MEMORY</b> specifies the amount of memory you want to allocate for a worker node if this value is not given the default value is the total memory available – 1G. Since we are running everything in our local machine we woundt want the slave the use up all our memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SPARK_WORKER_INSTANCES</b> specified the number of instances here its given as 2 since we will only create 2 slave nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SPARK_WORKER_DIR</b> will be the location that the run applications will run and which will include both logs and scratch space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>SPARK_WORKER_CORE</b> will specified the number of core will be use by the worker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add the IP address and port of the slaves into the following conf file"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "conf/slaves"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When using the launch scripts this file is used to identify the host-names of the machine that the slave nodes will be running, Here we have standalone machine so we set <i>localhost</i> in slaves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting Spark\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Start standalone Spark master server"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "./sbin/start-master.sh --webui-port 8090"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/spark-master-start-screen-shot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Starting up Spark Master WebUI on a custom port 8090, but default port is 8080."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now start workers for the master by the following commands"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "./sbin/start-slaves.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/spark_slaves.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Spark UI : open http://localhost:4040/ in browser. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Open Spark Shell\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "./bin/spark-shell spark://machine_name.local:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/spark_scala.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Clear the last line in Scala is CTRL-L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic Scala\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitting via command line."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/scala_hello_world.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submitting via submitted scripts. Create a directory for Scala code.  "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mkdir /usr/Local/Cellar/spark/code"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vi /usr/Local/Cellar/spark/code/HelloWorld.scala"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "object HelloWorld {\n",
    "   /* This is my first java program.  \n",
    "   * This will print 'Hello World' as the output\n",
    "   */\n",
    "   def main(args: Array[String]) {\n",
    "      println(\"Hello, world!\") // prints Hello World\n",
    "   }\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You cannot change directories per se within the Scala shell however you can do the following -"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "scala> System.getProperty(\"user.dir\")\n",
    "res9: String = /usr/local/Cellar/spark/1.6.2\n",
    "\n",
    "scala> System.setProperty(\"user.dir\",\"/usr/local/Cellar/spark/code\")\n",
    "res10: String = /usr/local/Cellar/spark/1.6.2\n",
    "\n",
    "scala> System.getProperty(\"user.dir\")\n",
    "res11: String = /usr/local/Cellar/spark/code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Submit and compile Scala code via terminal. "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    " scalac HelloWorld.scala\n",
    " scala HelloWorld"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/scala_compile.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install and Configure Apache Zeppelin\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>What is Apache Zeppelin?</b>  a new and incubating multi-purposed web-based notebook which brings data ingestion, data exploration, visualization, sharing and collaboration features to Hadoop and Spark."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will build Zeppelin binary from it’s source, so lets clone the git repository."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "git clone git@github.com:apache/incubator-zeppelin.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, compile the source code using Apache Maven.<br><br> <b>What is Apache Maven?</b> a software project management and comprehension tool. Based on the concept of a project object model (POM), Maven can manage a project's build, reporting and documentation from a central piece of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "mv /usr/Local/Cellar/incubator-zeppelin /usr/Local/Cellar/zeppelin\n",
    "cd /usr/Local/Cellar/zeppelin\n",
    "mvn clean package -Pspark-1.6 -Ppyspark -Dhadoop.version=2.6.0 -Phadoop-2.6 -DskipTests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For Zeppelin to connect to Spark Standalone Cluster, we need to make a couple of changes in the configuration of Zeppelin."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "mv conf/zeppelin-env.sh.template conf/zeppelin-env.sh\n",
    "vim conf/zeppelin-env.sh\n",
    "export MASTER=spark://GSs-MacBook-Pro.local:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It’s time to start Zeppelin web server."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "cd /usr/Local/Cellar/zeppelin\n",
    "./bin/zeppelin-daemon.sh start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/zeppelin-start.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure Spark Standalone Cluster in Zeppelin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to “Interpreter” and edit Spark “master” property with Spark Master’s URL. In my case it is spark://GSs-MacBook-Pro.local:7077. Don’t forget to click “Save” button."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/zeppelin-start-configuration.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Testing Zeppelin\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import scala.collection.JavaConversions._\n",
    "import scala.io.Source\n",
    "import org.apache.spark.SparkContext\n",
    "import org.apache.spark.SparkContext._\n",
    "import org.apache.spark.SparkConf\n",
    "\n",
    "import org.apache.spark.mllib.clustering.KMeans\n",
    "import org.apache.spark.mllib.linalg.Vectors\n",
    "\n",
    "println(\"Loading iris data from URL...\")\n",
    "val url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "val src = Source.fromURL(url).getLines.filter(_.size > 0).toList\n",
    "val textData = sc.parallelize(src)\n",
    "val parsedData = textData\n",
    "    .map(_.split(\",\").dropRight(1).map(_.toDouble))\n",
    "    .map(Vectors.dense(_)).cache()\n",
    "\n",
    "val numClusters = 3\n",
    "val numIterations = 20\n",
    "val clusters = KMeans.train(parsedData, numClusters, numIterations)\n",
    "\n",
    "// Evaluate clustering by computing Within Set Sum of Squared Errors\n",
    "val WSSSE = clusters.computeCost(parsedData)\n",
    "println(\"Within Set Sum of Squared Errors = \" + WSSSE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Credit: Code taken from book titled ‘Building a Recommendation Engine with Scala’"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/zeppelin-sample-test.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run and check success status of output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/zeppelin-sample-test-output.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopping All Services\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopping Spark Master"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "./sbin/stop-master.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopping Spark Slave"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "./sbin/stop-slave.sh spark://GSs-MacBook-Pro.local:7077"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Stopping Zeppelin"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "./bin/zeppelin-daemon.sh stop"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
