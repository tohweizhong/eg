{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SparkR Basics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD\n",
    "---\n",
    "Spark’s primary abstraction is a distributed collection of items called a Resilient Distributed Dataset (RDD). RDDs can be created from Hadoop InputFormats (such as HDFS files) or by transforming other RDDs. Let’s make a new RDD from the text of the README file in the SparkR-pkg source directory:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> textFile <- textFile(sc, \"/home/cloudera/SparkR-pkg/README.md\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDDs have actions, which return values, and transformations, which return pointers to new RDDs. Let’s start with a few actions:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> count(textFile)\n",
    "[1] 122\n",
    "\n",
    "> take(textFile, 1)\n",
    "[1] \"# R on Spark\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let’s use a transformation. We will use the filterRDD transformation to return a new RDD with a subset of the items in the file."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> linesWithSpark <- filterRDD(textFile, function(line){ grepl(\"Spark\", line)})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can chain together transformations and actions:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> count(filterRDD(textFile, function(line){ grepl(\"Spark\", line)})) # How many lines contain \"Spark\"?\n",
    "[1] 35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD Operations\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RDD actions and transformations can be used for more complex computations. Let’s say we want to find the line with the most words:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> reduce( lapply( textFile, function(line) { length(strsplit(unlist(line), \" \")[[1]])}), function(a, b) { if (a > b) { a } else { b }})\n",
    "[1] 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are two functions here: lapply and reduce. The inner function (lapply) maps a line to an integer value, creating a new RDD. The outer function (reduce) is called on the new RDD to find the largest line count. In this case, the arguments to both functions are passed as anonymous functions, but we can also define R functions beforehand and pass them as arguments to the RDD functions. For example, we’ll define a max function to make this code easier to understand:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> max <- function(a, b) {if (a > b) { a } else { b }}\n",
    "\n",
    "> reduce(map(textFile, function(line) { length(strsplit(unlist(line), \" \")[[1]])}), max)\n",
    "[1] 36"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One common data flow pattern is MapReduce, as popularized by Hadoop. MapReduce flows are easily implemented in SparkR:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> words <- flatMap(textFile,\n",
    "                 function(line) {\n",
    "                   strsplit(line, \" \")[[1]]\n",
    "                   })\n",
    "\n",
    "> wordCount <- lapply(words, function(word){ list(word, 1L) })\n",
    "\n",
    "> counts <- reduceByKey(wordCount, \"+\", 2L)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, we combined the flatMap, lapply and reduceByKey transformations to compute the per-word counts in the file as an RDD of (string, int) pairs. To collect the word counts in our shell, we can use the collect action:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> output <- collect(counts)\n",
    "\n",
    "> for (wordcount in output) {\n",
    "    cat(wordcount[[1]], \": \", wordcount[[2]], \"\\n\")\n",
    "  }\n",
    "\n",
    "SparkContext. :  1 \n",
    "SparkContext, :  1 \n",
    "master :  3 \n",
    "executors :  1 \n",
    "issues :  1 \n",
    "frontend :  1 \n",
    "variable :  3 \n",
    "[...]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Caching\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Spark also supports pulling data sets into a cluster-wide in-memory cache. This is very useful when data is accessed repeatedly, such as when querying a small “hot” dataset or when running an iterative algorithm like PageRank. As a simple example, let’s mark our linesWithSpark dataset to be cached:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> cache(linesWithSpark)\n",
    "\n",
    "> system.time(count(linesWithSpark))\n",
    "\n",
    "   user  system elapsed \n",
    "  0.955   0.225   2.127 \n",
    "\n",
    "> system.time(count(linesWithSpark))\n",
    "\n",
    "   user  system elapsed \n",
    "  0.945   0.188   1.078 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It may seem silly to use Spark to explore and cache a 100-line text file. The interesting part is that these same functions can be used on very large data sets, even when they are striped across tens or hundreds of nodes. You can also do this interactively by connecting the SparkR shell to a cluster, an example of which is described in the [SparkR on EC2 wiki page](https://github.com/amplab-extras/SparkR-pkg/wiki/SparkR-on-EC2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standalone Applications\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll walk through the process of writing and executing a standalone application in SparkR. As an example, we'll create a simple R script, SimpleApp.R:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "library(SparkR)\n",
    "\n",
    "sc <- sparkR.init(master=\"local\")\n",
    "\n",
    "logFile <- \"/home/cloudera/SparkR-pkg/README.md\"\n",
    "\n",
    "logData <- cache(textFile(sc, logFile))\n",
    "\n",
    "numAs <- count(filterRDD(logData, function(s) { grepl(\"a\", s) }))\n",
    "numBs <- count(filterRDD(logData, function(s) { grepl(\"b\", s) }))\n",
    "\n",
    "paste(\"Lines with a: \", numAs, \", Lines with b: \", numBs, sep=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This program just counts the number of lines containing ‘a’ and the number containing ‘b’ in a text file and returns the counts as a string on the command line. In this application, we use the sparkR.init() function to initialize a SparkContext which is then used to create RDDs. We can pass R functions to Spark where they are automatically serialized along with any variables they reference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run this application, execute the following from the SparkR-pkg directory:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "> ./sparkR examples/SimpleApp.R\n",
    "\n",
    "[1] \"Lines with a: 65, Lines with b: 32\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Other Examples\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In addition, SparkR includes several samples in the examples directory. To run one of them, use ./sparkR <filename> <args>. For example:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "./sparkR examples/pi.R local[2]"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "R",
   "language": "R",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r",
   "version": "3.3.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
