{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apache Pig Installation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify Installation\n",
    "---\n",
    "Pig is usually installed when you install Hadoop.  But to verify the installation of Apache Pig by typing the version command. If the installation is successful, you will get the version of Apache Pig as shown below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ pig –version \n",
    " \n",
    "Apache Pig version 0.15.0 (r1682971)  \n",
    "compiled Jun 01 2015, 11:44:35"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation of Apache Pig (Mac OS & Linux)\n",
    "---"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "brew install pig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/macpig.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation of Apache Pig (Windows)\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Download Apache Pig</b>\n",
    "First of all, download the latest version of Apache Pig from [here](http://www.apache.org/dyn/closer.cgi/pig).\n",
    "Step 1\n",
    "Open the homepage of Apache Pig website. Under the section News, click on the link release page as shown in the following snapshot."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![caption](files/index.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On the website you will have the source and binary files of Apache Pig in various distributions. Download the tar files of the source and binary files of Apache Pig 0.15, pig0.15.0-src.tar.gz."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Install Apache Pig</b><br>\n",
    "After downloading the Apache Pig software, install it in your Linux environment by following the steps given below.\n",
    "\n",
    "Create a directory with the name Pig in the same directory where the installation directories of Hadoop, Java, and other software were installed. (In our tutorial, we have created the Pig directory in the user named Hadoop)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ mkdir DRIVE:\\Globe\\Pig\\0.15.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Extract TAR Files</b><br>"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "$ tar zxvf pig-0.15.0-src.tar.gz "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Move Contents</b><br><br>\n",
    "Move the content of pig-0.15.0-src.tar.gz file to the Pig directory created earlier."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Configure Apache Pig\n",
    "---\n",
    "After installing Apache Pig, we have to configure it. To configure, we need to edit two files − .bashrc and pig.properties.\n",
    "\n",
    "In the .bashrc file, set the following variables −\n",
    "\n",
    "- PIG_HOME folder to the Apache Pig’s installation folder,\n",
    "\n",
    "- PATH environment variable to the bin folder, and\n",
    "\n",
    "- PIG_CLASSPATH environment variable to the etc (configuration) folder of your Hadoop installations (the directory that contains the core-site.xml, hdfs-site.xml and mapred-site.xml files)."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "export PIG_HOME = /home/Hadoop/Pig\n",
    "export PATH  = PATH:/home/Hadoop/pig/bin\n",
    "export PIG_CLASSPATH = $HADOOP_HOME/conf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>pig.properties file</b><br><br>\n",
    "In the conf folder of Pig, we have a file named pig.properties. In the pig.properties file, you can set various parameters as given below."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "pig -h properties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following properties are supported −"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Logging: verbose = true|false; default is false. This property is the same as -v\n",
    "       switch brief=true|false; default is false. This property is the same \n",
    "       as -b switch debug=OFF|ERROR|WARN|INFO|DEBUG; default is INFO.             \n",
    "       This property is the same as -d switch aggregate.warning = true|false; default is true. \n",
    "       If true, prints count of warnings of each type rather than logging each warning.\t\t \n",
    "\t\t \n",
    "Performance tuning: pig.cachedbag.memusage=<mem fraction>; default is 0.2 (20% of all memory).\n",
    "       Note that this memory is shared across all large bags used by the application.         \n",
    "       pig.skewedjoin.reduce.memusagea=<mem fraction>; default is 0.3 (30% of all memory).\n",
    "       Specifies the fraction of heap available for the reducer to perform the join.\n",
    "       pig.exec.nocombiner = true|false; default is false.\n",
    "           Only disable combiner as a temporary workaround for problems.         \n",
    "       opt.multiquery = true|false; multiquery is on by default.\n",
    "           Only disable multiquery as a temporary workaround for problems.\n",
    "       opt.fetch=true|false; fetch is on by default.\n",
    "           Scripts containing Filter, Foreach, Limit, Stream, and Union can be dumped without MR jobs.         \n",
    "       pig.tmpfilecompression = true|false; compression is off by default.             \n",
    "           Determines whether output of intermediate jobs is compressed.         \n",
    "       pig.tmpfilecompression.codec = lzo|gzip; default is gzip.\n",
    "           Used in conjunction with pig.tmpfilecompression. Defines compression type.         \n",
    "       pig.noSplitCombination = true|false. Split combination is on by default.\n",
    "           Determines if multiple small files are combined into a single map.         \n",
    "\t\t\t  \n",
    "       pig.exec.mapPartAgg = true|false. Default is false.             \n",
    "           Determines if partial aggregation is done within map phase, before records are sent to combiner.         \n",
    "       pig.exec.mapPartAgg.minReduction=<min aggregation factor>. Default is 10.             \n",
    "           If the in-map partial aggregation does not reduce the output num records by this factor, it gets disabled.\n",
    "\t\t\t  \n",
    "Miscellaneous: exectype = mapreduce|tez|local; default is mapreduce. This property is the same as -x switch\n",
    "       pig.additional.jars.uris=<comma seperated list of jars>. Used in place of register command.\n",
    "       udf.import.list=<comma seperated list of imports>. Used to avoid package names in UDF.\n",
    "       stop.on.failure = true|false; default is false. Set to true to terminate on the first error.         \n",
    "       pig.datetime.default.tz=<UTC time offset>. e.g. +08:00. Default is the default timezone of the host.\n",
    "           Determines the timezone used to handle datetime datatype and UDFs.\n",
    "           \n",
    "Additionally, any Hadoop property can be specified."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
